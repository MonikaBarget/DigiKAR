{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFsdtB/vTDJnWQpC52kLVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ieg-dhr/DigiKAR/blob/main/JupyterNotebooks_DigiKAR/Factoids_Step2a_VerticalConsolidation_Profs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a script for consolidating factoid lists in AP3.\n",
        "\n",
        "The package mainly uses the Pandas package in Python to read and manipulate EXCEL data as DataFrames. DataFrames are 2-dimensional data representations in rows and columns. They can be written to different file formats such as CSV, EXCEL, JSON or RDF.\n",
        "\n",
        "First of all, we need to connect this Colab notebook with your Google Drive and define the directory for input and output data.\n"
      ],
      "metadata": {
        "id": "S3ydZRhYATDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "directory=\"/content/drive/My Drive/Colab_DigiKAR/\""
      ],
      "metadata": {
        "id": "39qqRJOgZmPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the second step, we have to install additional Packages needed for working with CSV, EXCEL and DataFrames."
      ],
      "metadata": {
        "id": "dCAdylkZL9f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## install packages that are not part of Python's standard distribution\n",
        "\n",
        "!pip install xlsxwriter\n",
        "!pip install pandas\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "3d6OjjlTZ2ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 1**, we can import the packages to the script and load our data. Before merging the input files, names will be normalised as some have access spaces, capitalised surnames, or inverted first and last names.\n",
        "\n",
        "The combined data will be written to a new dataframe and displayed."
      ],
      "metadata": {
        "id": "agRRVOwiOWXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xlsxwriter\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "# path to input files\n",
        "\n",
        "factoid_path_API=\"https://github.com/ieg-dhr/DigiKAR/blob/main/Sample%20Data/Profs_API_factoid_consolidated_v1.xlsx?raw=true\"\n",
        "factoid_path_OCR=\"https://github.com/ieg-dhr/DigiKAR/blob/main/Sample%20Data/Profs_OCR_factoid_with-dates-and-institutions.xlsx?raw=true\"\n",
        "\n",
        "factoid_paths=[\"https://github.com/ieg-dhr/DigiKAR/blob/main/Sample%20Data/Profs_API_factoid_consolidated_v1.xlsx?raw=true\",\n",
        "               \"https://github.com/ieg-dhr/DigiKAR/blob/main/Sample%20Data/Profs_OCR_factoid_with-dates-and-institutions.xlsx?raw=true\"]\n",
        "\n",
        "# define dataframe for final output\n",
        "\n",
        "f_to_add=[]\n",
        "\n",
        "# structure of input files\n",
        "\n",
        "# obligatory columns in valid factoid list\n",
        "\n",
        "column_names = [\"factoid_ID\",\n",
        "                \"pers_ID\",\n",
        "                \"alternative_names\",\n",
        "                \"event_type\",\n",
        "                \"event_after-date\",\n",
        "                \"event_before-date\",\n",
        "                \"event_start\",\n",
        "                \"event_end\",\n",
        "                \"event_date\",\n",
        "                \"pers_title\",\n",
        "                \"pers_function\",\n",
        "                \"place_name\",\n",
        "                \"inst_name\",\n",
        "                \"rel_pers\",\n",
        "                \"source_quotations\",\n",
        "                \"additional_info\",\n",
        "                \"comment\",\n",
        "                \"info_dump\",\n",
        "                \"source\",\n",
        "                \"source_site\"]\n",
        "\n",
        "frame_list=[]\n",
        "for file in factoid_paths:\n",
        "    df = pd.read_excel(file, index_col=None, dtype=str) # axis=1, sort=False sheet_name='FactoidList'\n",
        "    df = df.fillna(\"n/a\") # replace empty fields for string\n",
        "    df_length=len(df)\n",
        "    person_cleaned=[]\n",
        "\n",
        "    try:\n",
        "      for x in range(0, df_length):\n",
        "          print(df_length - x)\n",
        "          e_df=df.iloc[[x]].fillna(\"@\") # virtual value to avoid issues with empty data frames\n",
        "\n",
        "          # CLEAN PERSON NAMES\n",
        "\n",
        "          pers_name=e_df['pers_name'].values[0]\n",
        "          print(pers_name)\n",
        "          if \",\" in pers_name:\n",
        "            pers_new=pers_name.split(\",\")\n",
        "            first_name=pers_new[1].strip()\n",
        "            #print(first_name)\n",
        "            last_name=pers_new[0].title().strip() # change \"all caps\" to sentence case\n",
        "            #print(last_name)\n",
        "            name_list=(first_name, last_name)\n",
        "\n",
        "            try:\n",
        "              name_reversed=\" \".join(name_list) # religious titles are being ignored\n",
        "              #print(\"This is the new name: \", name_reversed)\n",
        "            except Exception as e:\n",
        "              print(e)\n",
        "          else:\n",
        "            name_reversed=re.sub(\"\\s\\s+\" , \" \", pers_name)\n",
        "          #print(name_reversed)\n",
        "          person_cleaned.append(name_reversed)\n",
        "\n",
        "    except IndexError:\n",
        "      print(\"No more names found.\")\n",
        "\n",
        "    # add column with cleaned person names\n",
        "\n",
        "    print(person_cleaned)\n",
        "    print(len(person_cleaned))\n",
        "    df['pers_cleaned']=person_cleaned\n",
        "\n",
        "    frame_list.append(df)\n",
        "\n",
        "f = pd.concat(frame_list, axis=0, ignore_index=True, sort=False)\n",
        "\n",
        "print(\"There are \", len(f), \"items in your DataFrame!\")\n",
        "\n",
        "# delete all duplicate rows with exact matches\n",
        "\n",
        "f_unique=f.drop_duplicates()\n",
        "print(\"Your DataFrame has now \", len(f_unique), \"items with at least one unique cell.\" )\n",
        "\n",
        "display(f_unique)\n",
        "\n",
        "workbook=directory+'FACTOIDS_consolidated/Factoid_PROFS_merged_names-fixed.xlsx'\n",
        "print(workbook)\n",
        "writer = pd.ExcelWriter(workbook, engine='xlsxwriter') # create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "f_unique.to_excel(writer, sheet_name='ProfFactALL') # Convert the dataframe to an XlsxWriter Excel object.\n",
        "writer.save() # Close the Pandas Excel writer and output the Excel file.\n",
        "print(\"Done.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zYkO-hJ7rxxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 2**, we create dictionaries that define event successions. Some events start and end other events. Where the events started or ending or not yet made explicit, they are added to the dataframe in **step 3**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pFS6ouhczRj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create dictionaries with coded event relationships\n",
        "\n",
        "event_starting_dict= {\"Immatrikulation\":\"Studium\",\n",
        "                  \"Graduation\":\"Studium\",\n",
        "                  \"Aufnahme\":\"Mitgliedschaft\",\n",
        "                  \"Rezeption\":\"Mitgliedschaft\",\n",
        "                  \"Zulassung\":\"Studium\",\n",
        "                  \"Aufschwörung\":\"Funktionssausübung\",\n",
        "                  \"Introduktion\":\"Mitgliedschaft\",\n",
        "                  \"Vokation\":\"Funktionsausübung\",\n",
        "                  \"Ernennung\":\"Funktionsausübung\",\n",
        "                  \"Amtseinführung\":\"Funktionsausübung\",\n",
        "                  \"Ernennung\":\"Funktionsausübung\",\n",
        "                  \"Amtseinführung\":\"Funktionsausübung\",\n",
        "                  \"Amtsantritt\":\"Funktionsausübung\",\n",
        "                  \"Vereidigung\":\"Funktionsausübung\",\n",
        "                  \"Amtsantritt\":\"Funktionsausübung\",\n",
        "                  \"Beförderung\":\"Funktionsausübung\"}\n",
        "\n",
        "event_ending_dict= {\"Entlassung\":\"Funktionsausübung\",\n",
        "                  \"Suspendierung\":\"Funktionsausübung\",\n",
        "                  \"Absetzung\":\"Funktionsausübung\",\n",
        "                  \"Resignation\":\"Funktionsausübung\",\n",
        "                  \"Rücktritt\":\"Funktionsausübung\",\n",
        "                  \"Pensionierung\":\"Funktionsausübung\",\n",
        "                  \"Prüfungsverfahren\":\"Studium\",\n",
        "                  \"Absetzung\":\"Funktionsausübung\",\n",
        "                  \"Graduation\":\"Studium\"}"
      ],
      "metadata": {
        "id": "7n_pJlNK5vuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3** of the factoid consolidation includes the following operations:\n",
        "\n",
        "*   identify exact dates resulting from identical range dates\n",
        "*   add process events where only one-time events exist at present (see dictionary or events created in **step 2**)\n",
        "\n",
        "As a result of these operations, the dataframe will become larger as additional rows are added.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "srtsoNW4Mfln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open updated file with new person names from Github and create new dataframe\n",
        "\n",
        "factoid_path_PROFs=\"https://github.com/ieg-dhr/DigiKAR/blob/main/Sample%20Data/Factoid_PROFS_consolidation_STEP1_merged_names-fixed.xlsx?raw=true\"\n",
        "\n",
        "df2 = pd.read_excel(factoid_path_PROFs, index_col=None, dtype=str) # axis=1, sort=False sheet_name='FactoidList'\n",
        "#df2 = df2.fillna(\"n/a\") # replace empty fields for string\n",
        "df_size=len(df2)\n",
        "\n",
        "# add additional dates where applicable\n",
        "\n",
        "try:\n",
        "  for x in range(0, df_size):\n",
        "      print(df_size - x)\n",
        "      e_df=df2.iloc[[x]].fillna(\"@\") # virtual value to avoid issues with empty data frames\n",
        "\n",
        "      # SAME START/AFTER AND END/BEFORE DATE = EXACT DATE\n",
        "      # be careful to avoid chain-indexing in dataframe\n",
        "      # use multi-axis indexing (df.loc['a', '1']) instead\n",
        "\n",
        "\n",
        "      #f_unique['event_date'] = np.where(f_unique.event_start == f_unique.event_end, f_unique.event_start,\n",
        "                      #(np.where(f_unique.event_after-date== f_unique.event_before-date, f_unique.event_after-date,\n",
        "                      #<some_default_value>))\n",
        "\n",
        "      if e_df['event_start'].equals(e_df['event_end']):\n",
        "        new_date=e_df['event_start'].values[0]\n",
        "        #print(new_date)\n",
        "        df2.loc[x, 'event_date']=new_date\n",
        "      if e_df['event_after-date'].equals(e_df['event_before-date']):\n",
        "        new_date=e_df['event_after-date'].values[0]\n",
        "        #print(new_date)\n",
        "        df2.loc[x, 'event_date']=new_date\n",
        "      if len(e_df[\"event_date\"].values[0])>=4:\n",
        "        new_date=e_df[\"event_date\"].values[0]\n",
        "        df2.loc[x, 'event_start']=new_date\n",
        "        df2.loc[x, 'event_end']=new_date\n",
        "      else:\n",
        "        new_date=e_df['event_date'].values[0]\n",
        "        df2.loc[x, 'event_date']=new_date\n",
        "\n",
        "      # QUESTION @Florian: also create \"before\" and \"after\" dates for PROCESS EVENTS\n",
        "      # if only \"start\" and \"end\" dates are available so far?\n",
        "\n",
        "      # MATCH ONE-TIME EVENTS WITH PROCESS EVENTS\n",
        "\n",
        "      e_value=e_df[\"event_type\"].values[0]\n",
        "      if e_value in event_starting_dict:\n",
        "          #print(\"This event starts a process: \", e_value)\n",
        "          new_event=event_starting_dict[e_value]\n",
        "          if len(str(e_df['event_date'].values[0]))>=4:\n",
        "            new_start=e_df['event_date'].values[0]\n",
        "            new_after=\"n/a\"\n",
        "            new_end=\"n/a\"\n",
        "          elif len(str(e_df['event_start'].values[0]))>=4:\n",
        "            new_after=e_df['event_start'].values[0]\n",
        "            new_start=\"n/a\"\n",
        "            new_end=\"n/a\"\n",
        "          elif len(str(e_df['event_end'].values[0]))>=4:\n",
        "            new_start=e_df['event_after-date'].values[0]\n",
        "            new_after=\"n/a\"\n",
        "            new_end=\"n/a\"\n",
        "          elif len(str(e_df['event_before-date'].values[0]))>=4:\n",
        "            new_start=e_df['event_after-date'].values[0]\n",
        "            new_after=\"n/a\"\n",
        "            new_end=\"n/a\"\n",
        "          elif len(str(e_df['event_after-date'].values[0]))>=4:\n",
        "            new_after=e_df['event_after-date'].values[0]\n",
        "            new_start=\"n/a\"\n",
        "            new_end=\"n/a\"\n",
        "          else:\n",
        "            print(\"No valid date!\")\n",
        "            new_start=\"n/a\"\n",
        "            new_end=\"n/a\"\n",
        "\n",
        "          s=e_df.reindex(e_df.index.repeat(1))\n",
        "          s.loc[:,'event_type']=new_event # change the value\n",
        "          s.loc[:, 'factoid_ID']=\"reconstruction\" # change the value\n",
        "          s.loc[:, 'event_start']=new_start # change the value\n",
        "          s.loc[:, 'event_after-date']=new_after # change the value\n",
        "          s.loc[:, 'event_end']=new_end # change the value\n",
        "\n",
        "          f_to_add.append(s)\n",
        "          display(s)\n",
        "\n",
        "      elif e_value in event_ending_dict:\n",
        "          #print(\"This event ends a process: \", e_value)\n",
        "          new_event=event_ending_dict[e_value]\n",
        "          if len(str(e_df['event_date'].values[0]))>=4:\n",
        "            new_end=e_df['event_date'].values[0]\n",
        "            new_before=\"n/a\"\n",
        "            new_start=\"n/a\"\n",
        "          elif len(str(e_df['event_start'].values[0]))>=4:\n",
        "            new_before=e_df['event_start'].values[0]\n",
        "            new_end=\"n/a\"\n",
        "            new_start=\"n/a\"\n",
        "          elif len(str(e_df['event_end'].values[0]))>=4:\n",
        "            new_end=e_df['event_before-date'].values[0]\n",
        "            new_before=e_df['event_before-date'].values[0]\n",
        "            new_start=\"n/a\"\n",
        "          elif len(str(e_df['event_before-date'].values[0]))>=4:\n",
        "            new_before=e_df['event_after-date'].values[0]\n",
        "            new_end=\"n/a\"\n",
        "            new_start=\"n/a\"\n",
        "          elif len(str(e_df['event_after-date'].values[0]))>=4:\n",
        "            new_before=e_df['event_after-date'].values[0]\n",
        "            new_end=\"n/a\"\n",
        "            new_start=\"n/a\"\n",
        "          else:\n",
        "            print(\"No valid date!\")\n",
        "            new_end=\"n/a\"\n",
        "            new_start=\"n/a\"\n",
        "\n",
        "          s=e_df.reindex(e_df.index.repeat(1))\n",
        "          s.loc[:,'event_type']=new_event # change the value\n",
        "          s.loc[:, 'factoid_ID']=\"reconstruction\" # change the value\n",
        "          s.loc[:, 'event_start']=new_start # change the value\n",
        "          s.loc[:, 'event_after-date']=new_after # change the value\n",
        "          s.loc[:, 'event_end']=new_end # change the value\n",
        "\n",
        "          f_to_add.append(s)\n",
        "          display(s)\n",
        "\n",
        "      else:\n",
        "          print(\"No matching event.\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "display(f_to_add)\n",
        "print(len(f_to_add))"
      ],
      "metadata": {
        "id": "VH3urszuQciK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 4**, the updated data are written to an EXCEL file as a back-up and for archiving. In **steps 5 & 6**, the data will be manipulated further."
      ],
      "metadata": {
        "id": "q7gZNGGiu5Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.tseries.offsets import FY5253\n",
        "# write amended rows to existing data frame for further processing\n",
        "\n",
        "f_to_add.append(df2)\n",
        "\n",
        "df3 = pd.concat(f_to_add, axis=0, ignore_index=True, sort=False)\n",
        "\n",
        "print(len(df3))\n",
        "\n",
        "display(df3)\n",
        "\n",
        "workbook=directory+'FACTOIDS_consolidated/Factoid_PROFS_consolidation_STEP2_events-reconstructed_BACKUP.xlsx'\n",
        "print(workbook)\n",
        "writer = pd.ExcelWriter(workbook, engine='xlsxwriter') # create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "df3.to_excel(writer, sheet_name='FactCons1') # Convert the dataframe to an XlsxWriter Excel object.\n",
        "writer.save() # Close the Pandas Excel writer and output the Excel file.\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "3mCV6d9FTRBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the **step 5** of the consolidation, the script will try and identify similar events per person based on event type, place and institution. Those events are \"aggregated\" (that is: merged) while all source information etc. is preserved. In terms of dates, minimum and maximum dates given for the presumed identical events are used to create a new dataframe.\n",
        "\n",
        "The result of this process will be that the number of rows in our table structure will be more or less drastically decreased. Where the automated factoid aggregation based on four values is too radical, more columns can be included as obligatory matches before the data are merged."
      ],
      "metadata": {
        "id": "-Z-ubXWONf1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An alternative to working with the in-built aggregation function in Python is to create a hand-coded dictionary of event hierarchies and to use biographical proximities between events to decide which events are identical or succeed each other."
      ],
      "metadata": {
        "id": "Gzg54ZEKO-4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consolidate events per person\n",
        "\n",
        "f_new=df3\n",
        "\n",
        "# read person list\n",
        "\n",
        "pers_name_f=(f_new[['pers_name']])\n",
        "search_df=pers_name_f.drop_duplicates() # remove duplicates\n",
        "search_list=search_df['pers_name'].tolist()\n",
        "\n",
        "# count no. of entries in flattened person list\n",
        "\n",
        "no_person=len(search_list)\n",
        "print(\"There are\", no_person, \"unique person names in this data set.\")\n",
        "\n",
        "# iterate through unique persons to get their events\n",
        "\n",
        "frame_list=[]\n",
        "for name in search_list:\n",
        "    #print(\"\\n\",name, \"\\n\")\n",
        "    res_df=(f_new.loc[f_new['pers_name'] == name])\n",
        "\n",
        "# list existing events per person\n",
        "    bio_events=res_df['event_type'].values.tolist()\n",
        "    #print(set(bio_events))\n",
        "\n",
        "# check if duplicate events with same place and institution have different dates and create range\n",
        "\n",
        "    duplicate = res_df[res_df.duplicated(['event_type', 'pers_function', 'place_name', 'inst_name'])]\n",
        "    print(duplicate)\n",
        "    if len(duplicate)>1:\n",
        "        print(\"For \", name, \"there are \", len(duplicate), \"similar events.\")\n",
        "\n",
        "# aggregate similar events\n",
        "        try:\n",
        "          df_new = duplicate.groupby([\"event_type\", \"place_name\", \"inst_name\"]).agg( # This line of code merges cells!!\n",
        "                                        {\"event_after-date\":'min',\n",
        "                                        \"event_before-date\":'max',\n",
        "                                        \"event_start\":'min',\n",
        "                                        \"event_end\":'min',\n",
        "                                        \"factoid_ID\": list, # ORIGINAL IDS are combined as RECONSTRUCTION MARKER\n",
        "                                        \"pers_ID\":list,\n",
        "                                        \"pers_name\":list,\n",
        "                                        \"alternative_names\":list,\n",
        "                                        \"pers_title\":list,\n",
        "                                        \"pers_function\":list,\n",
        "                                        \"inst_name\":list,\n",
        "                                        \"rel_pers\":list,\n",
        "                                        \"source_quotations\":list,\n",
        "                                        \"additional_info\":list,\n",
        "                                        \"comment\":list,\n",
        "                                        \"info_dump\":list,\n",
        "                                        \"source\":list,\n",
        "                                        \"source_site\":list})\n",
        "          frame_list.append(df_new)\n",
        "        except TypeError:\n",
        "          print(\"One of the date fields contains invalid characters / is string!\")\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "frame_list.append(f_new)\n",
        "\n",
        "f_result = pd.concat(frame_list, axis=0, ignore_index=False, sort=False)\n"
      ],
      "metadata": {
        "id": "6t0WjpYtIeA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add event values from dict to data frame\n",
        "\n",
        "try:\n",
        "    f_result['event_value'] = f_result['event_type'].map(event_value_dict)\n",
        "    f_result.sort_values(by =['event_after-date','event_start','event_before-date', 'event_end', 'event_value'])\n",
        "except:\n",
        "    print(\"No values.\")\n",
        "\n",
        "print(\"Aggregation complete!\")\n",
        "\n",
        "display(f_result)\n",
        "\n",
        "# find events with no dates at all and reconstruct before & after dates based on hierarchies\n"
      ],
      "metadata": {
        "id": "iL7uHngsU59m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step is to write the results to a single output file."
      ],
      "metadata": {
        "id": "8wRkCp062_jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write all results to new EXCEL file\n",
        "\n",
        "workbook=directory+'FACTOIDS_consolidated/Factoid_PROFS_consolidation_STEP3_aggregation-hierarchisation_NEW.xlsx'\n",
        "print(workbook)\n",
        "writer = pd.ExcelWriter(workbook, engine='xlsxwriter') # create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "f_result.to_excel(writer, sheet_name='FactCons') # Convert the dataframe to an XlsxWriter Excel object.\n",
        "writer.save() # Close the Pandas Excel writer and output the Excel file.\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "qmxPBH9Q3FY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output files and repeat process if necessary.\n",
        "\n",
        "Script by Monika Barget, Maastricht/Mainz\n",
        "\n",
        "January 2023\n"
      ],
      "metadata": {
        "id": "GBSEVpnKXS_u"
      }
    }
  ]
}