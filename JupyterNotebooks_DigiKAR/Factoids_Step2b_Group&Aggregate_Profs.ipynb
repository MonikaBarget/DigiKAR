{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRZDqYYmo9WpO06b1gYkGo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a script for **grouping and aggregating events** from AP3 factoid lists in the DigiKAR project.\n",
        "\n",
        "Grouping, in this case, means that rows of a column with the same values are combined according to a rule specified in the function. This rule can be a simple list of all existing values, a minimum value, a maximium value, or a sum.\n",
        "\n",
        "This script performs grouping and aggregation with the Pandas package in Python to read and manipulate EXCEL data as DataFrames. DataFrames are 2-dimensional data representations in rows and columns. They can be written to different file formats such as CSV, EXCEL, JSON or RDF.\n",
        "\n",
        "First of all, we need to connect the Colab notebook with Google Drive and define the directory for input and output data."
      ],
      "metadata": {
        "id": "S3ydZRhYATDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "directory=\"/content/drive/My Drive/Colab_DigiKAR/\""
      ],
      "metadata": {
        "id": "39qqRJOgZmPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the second step, we have to install additional Packages needed for working with CSV, EXCEL and DataFrames."
      ],
      "metadata": {
        "id": "dCAdylkZL9f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## install packages that are not part of Python's standard distribution\n",
        "\n",
        "!pip install xlsxwriter\n",
        "!pip install pandas\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "3d6OjjlTZ2ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 1**, we can import the packages to the script and load our data. Before merging the input files, names will be normalised as some have access spaces, capitalised surnames, or inverted first and last names.\n",
        "\n",
        "The combined data will be written to a new dataframe and displayed."
      ],
      "metadata": {
        "id": "agRRVOwiOWXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xlsxwriter\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "# path to input files\n",
        "\n",
        "factoid_path1=\"\" # ADD NEW PROF DATA\n",
        "\n",
        "factoid_paths=[factoid_path1] # ADD MORE URLS IF NECESSARY\n",
        "\n",
        "# define dataframe for final output\n",
        "\n",
        "f_to_add=[]\n",
        "\n",
        "# structure of input files\n",
        "\n",
        "# obligatory columns in valid factoid list\n",
        "\n",
        "column_names = [\"factoid_ID\",\n",
        "                \"pers_ID\",\n",
        "                \"alternative_names\",\n",
        "                \"event_type\",\n",
        "                \"event_after-date\",\n",
        "                \"event_before-date\",\n",
        "                \"event_start\",\n",
        "                \"event_end\",\n",
        "                \"event_date\",\n",
        "                \"pers_title\",\n",
        "                \"pers_function\",\n",
        "                \"place_name\",\n",
        "                \"inst_name\",\n",
        "                \"rel_pers\",\n",
        "                \"source_quotations\",\n",
        "                \"additional_info\",\n",
        "                \"comment\",\n",
        "                \"info_dump\",\n",
        "                \"source\",\n",
        "                \"source_site\"]\n",
        "\n",
        "frame_list=[]\n",
        "for file in factoid_paths:\n",
        "    df = pd.read_excel(file, index_col=None, dtype=str) # axis=1, sort=False sheet_name='FactoidList'\n",
        "    df = df.fillna(\"n/a\") # replace empty fields for string\n",
        "    frame_list.append(df)\n",
        "\n",
        "f = pd.concat(frame_list, axis=0, ignore_index=True, sort=False)\n",
        "\n",
        "print(\"There are \", len(f), \"items in your DataFrame!\")\n",
        "\n",
        "# delete all duplicate rows with exact matches\n",
        "\n",
        "f_unique=f.drop_duplicates()\n",
        "print(\"Your DataFrame has now \", len(f_unique), \"items with at least one unique cell.\" )\n",
        "\n",
        "display(f_unique)\n"
      ],
      "metadata": {
        "id": "zYkO-hJ7rxxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the **step 2** of the consolidation, the script will try and identify similar events per person based on event type, place and institution. Those events are \"aggregated\" (that is: merged) while all source information etc. is preserved. In terms of dates, minimum and maximum dates given for the presumed identical events are used to create a new dataframe.\n",
        "\n",
        "The result of this process will be that the number of rows in our table structure will be more or less drastically decreased. Where the automated factoid aggregation based on four values is too radical, more columns can be included as obligatory matches before the data are merged."
      ],
      "metadata": {
        "id": "-Z-ubXWONf1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consolidate events per person\n",
        "\n",
        "f_new=f_unique\n",
        "\n",
        "# read person list\n",
        "\n",
        "pers_name_f=(f_new[['pers_name']])\n",
        "search_df=pers_name_f.drop_duplicates() # remove duplicates\n",
        "search_list=search_df['pers_name'].tolist()\n",
        "\n",
        "# count no. of entries in flattened person list\n",
        "\n",
        "no_person=len(search_list)\n",
        "print(\"There are\", no_person, \"unique person names in this data set.\")\n",
        "\n",
        "# iterate through unique persons to get their events\n",
        "\n",
        "frame_list=[]\n",
        "for name in search_list:\n",
        "    #print(\"\\n\",name, \"\\n\")\n",
        "    res_df=(f_new.loc[f_new['pers_name'] == name])\n",
        "\n",
        "# list existing events per person\n",
        "    bio_events=res_df['event_type'].values.tolist()\n",
        "    #print(set(bio_events))\n",
        "\n",
        "# check if duplicate events with same place and institution have different dates and create range\n",
        "\n",
        "    duplicate = res_df[res_df.duplicated(['event_type', 'pers_function', 'place_name', 'inst_name'])]\n",
        "    print(duplicate)\n",
        "    if len(duplicate)>1:\n",
        "        print(\"For \", name, \"there are \", len(duplicate), \"similar events.\")\n",
        "\n",
        "# aggregate similar events\n",
        "        try:\n",
        "          df_new = duplicate.groupby([\"event_type\", \"place_name\", \"inst_name\"]).agg( # This line of code merges cells!!\n",
        "                                        {\"event_after-date\":'min',\n",
        "                                        \"event_before-date\":'max',\n",
        "                                        \"event_start\":'min',\n",
        "                                        \"event_end\":'min',\n",
        "                                        \"factoid_ID\": list, # ORIGINAL IDS are combined as RECONSTRUCTION MARKER\n",
        "                                        \"pers_ID\":list,\n",
        "                                        \"pers_name\":list,\n",
        "                                        \"alternative_names\":list,\n",
        "                                        \"pers_title\":list,\n",
        "                                        \"pers_function\":list,\n",
        "                                        \"inst_name\":list,\n",
        "                                        \"rel_pers\":list,\n",
        "                                        \"source_quotations\":list,\n",
        "                                        \"additional_info\":list,\n",
        "                                        \"comment\":list,\n",
        "                                        \"info_dump\":list,\n",
        "                                        \"source\":list,\n",
        "                                        \"source_site\":list})\n",
        "          frame_list.append(df_new)\n",
        "        except TypeError:\n",
        "          print(\"One of the date fields contains invalid characters / is string!\")\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "frame_list.append(f_new)\n",
        "\n",
        "f_result = pd.concat(frame_list, axis=0, ignore_index=False, sort=False)\n"
      ],
      "metadata": {
        "id": "6t0WjpYtIeA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 3**, the updated data are written to an EXCEL file as a back-up and for archiving. In **steps 5 & 6**, the data will be manipulated further."
      ],
      "metadata": {
        "id": "q7gZNGGiu5Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.tseries.offsets import FY5253\n",
        "# write amended rows to existing data frame for further processing\n",
        "\n",
        "f_to_add.append(df2)\n",
        "\n",
        "df3 = pd.concat(f_to_add, axis=0, ignore_index=True, sort=False)\n",
        "\n",
        "print(len(df3))\n",
        "\n",
        "display(df3)\n",
        "\n",
        "workbook=directory+'FACTOIDS_consolidated/Factoid_PROFS_consolidation_STEP2_events-reconstructed_BACKUP.xlsx'\n",
        "print(workbook)\n",
        "writer = pd.ExcelWriter(workbook, engine='xlsxwriter') # create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "df3.to_excel(writer, sheet_name='FactCons1') # Convert the dataframe to an XlsxWriter Excel object.\n",
        "writer.save() # Close the Pandas Excel writer and output the Excel file.\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "3mCV6d9FTRBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 5**, we create another dictionary to rank events. This time, the events are given a value between 0 and 100 to define at what stages in people's biographies they normally occur.\n",
        "\n",
        "**We also define which events generally occur before the others!**"
      ],
      "metadata": {
        "id": "zs3VF7uiUHcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ranking of events if no time is given\n",
        "\n",
        "event_value_dict={\"Sonstiges\":0,\n",
        "                  \"Geburt\":1,\n",
        "                  \"Taufe\":2,\n",
        "                  \"Primäre Bildungsstation\":3,\n",
        "                  \"Privatunterricht\":3,\n",
        "                  \"Rezeption\":10,\n",
        "                  \"Zulassung\":10,\n",
        "                  \"Immatrikulation\":10,\n",
        "                  \"Studium\":10,\n",
        "                  \"Prüfungsverfahren\":10,\n",
        "                  \"Graduation\":10,\n",
        "                  \"Praktikum\":10,\n",
        "                  \"Promotion\":10,\n",
        "                  \"Wohnsitznahme\": 10,\n",
        "                  \"Reise\":20,\n",
        "                  \"Nobilitierung\":20,\n",
        "                  \"Aufnahme\":20,\n",
        "                  \"Aufschwörung\":20,\n",
        "                  \"Eheschließung\":20,\n",
        "                  \"Funktionsausübung\":20,\n",
        "                  \"erfolglose Bewerbung\":20,\n",
        "                  \"Rejektion\":20,\n",
        "                  \"Aufenthalt\":20,\n",
        "                  \"mittelbare Nobilitierung\":20,\n",
        "                  \"Privilegierung\":20,\n",
        "                  \"Wappenbesserung\":20,\n",
        "                  \"Introduktion\":30,\n",
        "                  \"Mitgliedschaft\":30,\n",
        "                  \"Gesandtschaft\":30,\n",
        "                  \"Präsentation\":30,\n",
        "                  \"Vokation\":39,\n",
        "                  \"Ernennung\":40,\n",
        "                  \"Amtseinführung\":41,\n",
        "                  \"Vereidigung\":41,\n",
        "                  \"Amtsantritt\":42,\n",
        "                  \"Beförderung\":44,\n",
        "                  \"Ehrung\":45,\n",
        "                  \"Entlassung\":50,\n",
        "                  \"Suspendierung\":50,\n",
        "                  \"Absetzung\":50,\n",
        "                  \"Resignation\":50,\n",
        "                  \"Rücktritt\":50,\n",
        "                  \"Pensionierung\":90,\n",
        "                  \"Pension\":91,\n",
        "                  \"Tod\":100}\n",
        "\n",
        "event_hierarchy_dict={\n",
        "                  \"Geburt\":\"Taufe\",\n",
        "                  \"Geburt\":\"Tod\",\n",
        "                  \"Geburt\":\"Taufe\",\n",
        "                  \"Geburt\":\"Taufe\",\n",
        "                  \"Geburt\":\"Taufe\",\n",
        "                  \"Geburt\":\"Taufe\",\n",
        "                  \"Primäre Bildungsstation\":\"Studium\",\n",
        "                  \"Immatrikulation\":\"Studium\",\n",
        "                  \"Prüfungsverfahren\":\"Graduation\",\n",
        "                  \"Studium\":\"Promotion\",\n",
        "                  \"Aufnahme\":\"Funktionsausübung\",\n",
        "                  \"Aufschwörung\":\"Funktionssausübung\",\n",
        "                  \"erfolglose Bewerbung\":\"Funktionssausübung\",\n",
        "                  \"Introduktion\":\"Mitgliedschaft\",\n",
        "                  \"Vokation\":\"Funktionsausübung\",\n",
        "                  \"Ernennung\":\"Funktionsausübung\",\n",
        "                  \"Amtseinführung\":\"Funktionsausübung\",\n",
        "                  \"Vereidigung\":\"Funktionsausübung\",\n",
        "                  \"Amtsantritt\":\"Funktionsausübung\",\n",
        "                  \"Beförderung\":\"Funktionsausübung\",\n",
        "                  \"Funktionsausübung\":\"Entlassung\",\n",
        "                  \"Funktionsausübung\":\"Suspendierung\",\n",
        "                  \"Funktionsausübung\":\"Absetzung\",\n",
        "                  \"Funktionsausübung\":\"Resignation\",\n",
        "                  \"Funktionsausübung\":\"Rücktritt\",\n",
        "                  \"Funktionsausübung\":\"Pensionierung\",\n",
        "                  \"Funktionsausübung\":\"Pension\",\n",
        "                  \"Funktionsausübung\":\"Tod\"}"
      ],
      "metadata": {
        "id": "HzfdsC-CSufh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add event values from dict to data frame\n",
        "\n",
        "try:\n",
        "    f_result['event_value'] = f_result['event_type'].map(event_value_dict)\n",
        "    f_result.sort_values(by =['event_after-date','event_start','event_before-date', 'event_end', 'event_value'])\n",
        "except:\n",
        "    print(\"No values.\")\n",
        "\n",
        "print(\"Aggregation complete!\")\n",
        "\n",
        "display(f_result)\n",
        "\n",
        "# find events with no dates at all and reconstruct before & after dates based on hierarchies\n"
      ],
      "metadata": {
        "id": "iL7uHngsU59m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step is to write the results to a single output file."
      ],
      "metadata": {
        "id": "8wRkCp062_jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write all results to new EXCEL file\n",
        "\n",
        "workbook=directory+'FACTOIDS_consolidated/Factoid_PROFS_consolidation_STEP3_aggregation-hierarchisation_NEW.xlsx'\n",
        "print(workbook)\n",
        "writer = pd.ExcelWriter(workbook, engine='xlsxwriter') # create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "f_result.to_excel(writer, sheet_name='FactCons') # Convert the dataframe to an XlsxWriter Excel object.\n",
        "writer.save() # Close the Pandas Excel writer and output the Excel file.\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "qmxPBH9Q3FY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output files and repeat process if necessary.\n",
        "\n",
        "Script by Monika Barget, Maastricht/Mainz\n",
        "\n",
        "January 2023\n"
      ],
      "metadata": {
        "id": "GBSEVpnKXS_u"
      }
    }
  ]
}