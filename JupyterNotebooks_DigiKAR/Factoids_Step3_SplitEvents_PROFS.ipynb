{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a script for splitting multiple events per cell in the OCR + API Prof Data collected for DigiKAR. We will use the [pandas package in Python](https://pandas.pydata.org/) to read the data from an EXCEL file and manipulate them. **STEP 1** is to connect Google Colab with the user's Google Drive.\n"
      ],
      "metadata": {
        "id": "S3ydZRhYATDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "directory=\"/content/drive/My Drive/Colab_DigiKAR/\""
      ],
      "metadata": {
        "id": "39qqRJOgZmPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **STEP 2**, we have to install additional Packages needed for working with CSV, EXCEL and DataFrames. The [ast](https://docs.python.org/3/library/ast.html) package can be used for converting cell content in a dataframe to strings or lists. "
      ],
      "metadata": {
        "id": "dCAdylkZL9f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## install packages that are not part of Python's standard distribution\n",
        "\n",
        "!pip install xlsxwriter\n",
        "!pip install pandas\n",
        "!pip install ast"
      ],
      "metadata": {
        "id": "3d6OjjlTZ2ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 3**, we can import the packages to the script and load our data. Before merging the input files, names will be normalised as some have access spaces, capitalised surnames, or inverted first and last names.\n",
        "\n",
        "The combined data will be written to a new dataframe and displayed as a fully searchable table."
      ],
      "metadata": {
        "id": "agRRVOwiOWXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xlsxwriter\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import os\n",
        "# from ast import literal_eval # can be used to check if cell content is string but different function was used below \n",
        "\n",
        "# path to input files\n",
        "\n",
        "file_path=\"https://github.com/ieg-dhr/DigiKAR/blob/main/Sample%20Data/Factoid_PROFS_consolidation_STEP2_events-reconstructed.xlsx?raw=true\"\n",
        "\n",
        "# define dataframe for final output\n",
        "\n",
        "f_to_add=[]\n",
        "\n",
        "# structure of input files\n",
        "\n",
        "# obligatory columns in valid factoid list\n",
        "\n",
        "column_names = [\"factoid_ID\",\n",
        "                \"pers_ID\",\n",
        "                \"alternative_names\",\n",
        "                \"event_type\",\n",
        "                \"event_after-date\",\n",
        "                \"event_before-date\",\n",
        "                \"event_start\",\n",
        "                \"event_end\",\n",
        "                \"event_date\",\n",
        "                \"pers_title\",\n",
        "                \"pers_function\",\n",
        "                \"place_name\",\n",
        "                \"inst_name\",\n",
        "                \"rel_pers\",\n",
        "                \"source_quotations\",\n",
        "                \"additional_info\",\n",
        "                \"comment\",\n",
        "                \"info_dump\",\n",
        "                \"source\",\n",
        "                \"source_site\"]\n",
        "                \n",
        "\n",
        "\n",
        "df = pd.read_excel(file_path, index_col=None, dtype=str) # axis=1, sort=False sheet_name='FactoidList'\n",
        "df = df.fillna(\"n/a\") # replace empty fields for string\n",
        "df_length=len(df)\n",
        "\n",
        "print(\"There are \", len(df), \"items in your DataFrame!\")\n",
        "\n",
        "# delete all duplicate rows with exact matches\n",
        "\n",
        "df_unique=df.drop_duplicates()\n",
        "print(\"Your DataFrame has now \", len(df_unique), \"items with at least one unique cell.\" )\n",
        "\n",
        "display(df_unique)\n"
      ],
      "metadata": {
        "id": "zYkO-hJ7rxxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 4**, we will multiply (\"explode\") entries in the pers_function column. This solution was kindly suggested by [Lukas Hstermeyer](https://stackoverflow.com/users/5240684/lukas-hestermeyer) on StackOverflow to avoid using an `if` loop within a `for` loop. In general, loops for iterating over dataframes out to by avoided in [Pandas](https://pandas.pydata.org/). It is faster and more reliable to use [vectorized solutions](https://towardsdatascience.com/you-dont-always-have-to-loop-through-rows-in-pandas-22a970b347ac).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pFS6ouhczRj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df_unique\n",
        "print(\"The original frame has \", len(df2), \"entries.\")\n",
        "\n",
        "# make sure all values are strings\n",
        "df2 = df2.astype(str)\n",
        "df2['pers_function'] = df2['pers_function'].str.split(', ')\n",
        "\n",
        "df_explode=df2.explode('pers_function')\n",
        "\n",
        "print(\"The new frame has \", len(df_explode), \"entries.\")\n",
        "\n",
        "display(df_explode)"
      ],
      "metadata": {
        "id": "Bl8-w3Fxy9V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 5**, we write all processed data to a new EXCEL file."
      ],
      "metadata": {
        "id": "q7gZNGGiu5Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write updated dataframe to a new EXCEL file\n",
        "\n",
        "workbook=directory+'FACTOIDS_consolidated/Factoid_PROFS_consolidation_STEP3_events-split.xlsx'\n",
        "print(workbook)\n",
        "writer = pd.ExcelWriter(workbook, engine='xlsxwriter') # create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "df_explode.to_excel(writer, sheet_name='FactProfSplit') # Convert the dataframe to an XlsxWriter Excel object.\n",
        "writer.save() # Close the Pandas Excel writer and output the Excel file.\n",
        "print(\"Done.\") \n",
        " "
      ],
      "metadata": {
        "id": "3mCV6d9FTRBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output files and repeat process if necessary.\n",
        "\n",
        "Script by Monika Barget, Maastricht/Mainz\n",
        "\n",
        "March 2023\n"
      ],
      "metadata": {
        "id": "GBSEVpnKXS_u"
      }
    }
  ]
}
