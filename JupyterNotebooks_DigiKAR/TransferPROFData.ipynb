{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a script for transferring PROF & STUDENT data from TXT to EXCEL while identifying known entities from the DigiKAR ontology lists.\n",
        "\n",
        "First of all, we need to connect this Colab notebook with your Google Drive and define the directory for input and output data.\n"
      ],
      "metadata": {
        "id": "S3ydZRhYATDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "directory=\"/content/drive/My Drive/Colab_DigiKAR/\""
      ],
      "metadata": {
        "id": "39qqRJOgZmPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the second step, we have to install additional Packages needed for working with CSV, EXCEL and DataFrames."
      ],
      "metadata": {
        "id": "dCAdylkZL9f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## install packages that are not part of Python's standard distribution\n",
        "\n",
        "!pip install xlsxwriter\n",
        "!pip install pandas\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "3d6OjjlTZ2ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can import the packages to the script and load our data."
      ],
      "metadata": {
        "id": "agRRVOwiOWXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## import relevant packages\n",
        "\n",
        "import xlsxwriter\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "-5cFcvKSr347"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to read the ontology files from either a directory or GITHUB. We read their content to a dataframe (table structure) that we can use to identify substrings in the input data."
      ],
      "metadata": {
        "id": "WiLBP5PWr5pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define files containing ontological mapping\n",
        "\n",
        "event_ontology='https://raw.githubusercontent.com/ieg-dhr/DigiKAR/main/OntologyFiles/event_ontology.csv' \n",
        "title_ontology='https://github.com/ieg-dhr/DigiKAR/blob/main/OntologyFiles/titles_ontology.csv?raw=true' \n",
        "function_ontology='https://github.com/ieg-dhr/DigiKAR/blob/main/OntologyFiles/functions_ontology.csv?raw=true'\n",
        "place_ontology='https://raw.githubusercontent.com/ieg-dhr/DigiKAR/main/OntologyFiles/place_ontology.csv' \n",
        "\n",
        "# open ontology files and read content to dataframes\n",
        "\n",
        "# READ EVENTS\n",
        "data_e = pd.read_csv(event_ontology, sep=\",\")\n",
        "events_old=data_e['event_old'].values.tolist()\n",
        "    \n",
        "# READ TITLES\n",
        "data_t = pd.read_csv(title_ontology, sep=\";\")\n",
        "title_old=data_t['title_old'].values.tolist()\n",
        "\n",
        "# READ FUNCTIONS\n",
        "data_f = pd.read_csv(function_ontology, sep=\";\")\n",
        "function_old=data_f['function_old'].values.tolist()\n",
        "\n",
        "# READ PLACES\n",
        "data_p = pd.read_csv(place_ontology, sep=\",\")\n",
        "places_old=data_p['place_old'].values.tolist()\n"
      ],
      "metadata": {
        "id": "spKIUGXb_TZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define and execute the function that reads information from a semi-structured TXT file with out OCR data to another data frame. The preliminary columns we assign are \"name\", \"info\" and \"source\". Then we split up the \"info\" into several individual events. For this process, we use the split() function in Python and several delimiters: #PERSON, #SOURCE and the semi-colon \";\"."
      ],
      "metadata": {
        "id": "4CoV-xfqsjO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to process data\n",
        "def extract_information(filenames):\n",
        "        \n",
        "# read files in directory to EXCEL\n",
        "    name_list=[]\n",
        "    source_list=[]\n",
        "    info_list=[]\n",
        "    for item in os.listdir(filenames):\n",
        "        # using readlines()\n",
        "        with open(os.path.join(filenames, item)) as f:\n",
        "          contents=f.read()\n",
        "          #print(contents[:500])\n",
        "          # split on hashtag to extract entries for each person\n",
        "          persons=contents.split(\"#PERSON\")\n",
        "          # check substrings\n",
        "          for p in persons:\n",
        "            source=p.split(\"#SOURCE\")\n",
        "            try:\n",
        "              p_new=source[0]\n",
        "              s_new=source[1]\n",
        "          # separate name from events\n",
        "              events=p_new.split(';')\n",
        "              n_items=len(events)\n",
        "              name_p=[events[0]] * (n_items-1)\n",
        "              source_p=[s_new] * (n_items-1)\n",
        "              for n in name_p:\n",
        "                name_list.append(n)\n",
        "              for s in source_p:\n",
        "                source_list.append(s)\n",
        "              for e in events[1:(n_items)]:\n",
        "                #print(\"ITEM: \", events[0], \":\", e)\n",
        "                info_list.append(e)\n",
        "            except IndexError:\n",
        "              continue\n",
        "\n",
        "          # add data to new dataframe\n",
        "\n",
        "          global df_size\n",
        "          df_size = len(info_list)\n",
        "          #print(len(name_list)) # to check that all lists have the same length\n",
        "          #print(len(source_list)) # to check that all lists have the same length\n",
        "          \n",
        "          data = {\n",
        "              'Name': name_list,\n",
        "              'Info': info_list,\n",
        "              'Source': source_list\n",
        "                }\n",
        "\n",
        "          event_df = pd.DataFrame(data)\n",
        "          \n",
        "# show data frame if small enough\n",
        "try:\n",
        "    display(event_df) \n",
        "except:\n",
        "    display(event_df[:100]) # only show first 100 rows\n",
        "\n",
        "# call function with input\n",
        "\n",
        "filenames = directory+\"ReadTXTtoEXCEL\"\n",
        "extract_information(filenames)\n"
      ],
      "metadata": {
        "id": "3B8Ce6kFToGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above section of the script creates a link between each name and individual events associated with that name, e.g. \n",
        "\n",
        "```\n",
        "NAME: ZURMÃœHLEN, Paulus Josephus Ignatius\n",
        "EVENT: cand. jur., 24.9.1751\n",
        "```\n",
        "\n",
        "Our next script section attempts to identify known entities in the event string and write relevant information to \"event\", \"title\" or \"function\" columns in the existing data frame. This is difficult because a lot of Latin expressions and abbreviations in both Latin and German have been used."
      ],
      "metadata": {
        "id": "Ewa2Wc29tNNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from traitlets.config.application import T\n",
        "# Check INFO column in existing DF\n",
        "place_list=[]\n",
        "title_list=[]\n",
        "event_list=[]\n",
        "function_list=[]\n",
        "\n",
        "try:\n",
        "  for x in range(0, df_size):\n",
        "      e_df=event_df.iloc[[x]]\n",
        "      #print(\"NEW FRAME: \", e_df)\n",
        "      event_x_list=[]\n",
        "      title_x_list=[]\n",
        "      place_x_list=[]\n",
        "      function_x_list=[]\n",
        "\n",
        "    # FIND EVENT\n",
        "      for e_old in events_old:\n",
        "          e_values=e_df[\"Info\"].values[0]\n",
        "          if e_old in e_values:\n",
        "              print(\"MATCH EVENT: \", e_old)\n",
        "              event_x_list.append(e_old)\n",
        "          else:\n",
        "              event_x_list.append(\"n/a\")\n",
        "      event_list.append(set(event_x_list))\n",
        "\n",
        "    # FIND TITLE\n",
        "      for t_old in title_old:\n",
        "          t_values=e_df[\"Info\"].values[0]\n",
        "          if t_old in t_values:\n",
        "              print(\"MATCH TITLE: \", t_old)\n",
        "              title_x_list.append(e_old)\n",
        "          else:\n",
        "              title_x_list.append(\"n/a\")\n",
        "      title_list.append(set(title_x_list))\n",
        "\n",
        "    # FIND PLACES\n",
        "      for p in places_old:\n",
        "          if p in e_df[\"Info\"].values[0]:\n",
        "              print(\"MATCH PLACE: \", p)\n",
        "              place_x_list.append(p)\n",
        "          else:\n",
        "              place_x_list.append(\"n/a\")\n",
        "      place_list.append(set(place_x_list))\n",
        "                \n",
        "    # FIND FUNCTION\n",
        "      for f_old in function_old:\n",
        "          if f_old in e_df[\"Info\"].values[0]:\n",
        "              print(\"MATCH FUNCTION: \", f_old)\n",
        "              function_x_list.append(f_old)\n",
        "          else:\n",
        "              function_x_list.append(\"n/a\")\n",
        "      function_list.append(set(function_x_list))\n",
        "\n",
        "except IndexError:\n",
        "  print(\"No more data found.\")\n",
        "\n",
        "# add new data to existing data frame\n",
        "event_df[\"events\"] = event_list\n",
        "event_df[\"places\"] = place_list\n",
        "event_df[\"titles\"] = title_list\n",
        "event_df[\"functions\"] = function_list\n",
        "\n",
        "display(event_df[:100])\n",
        "\n",
        "# write all results to new EXCEL file\n",
        "\n",
        "workbook=directory+'Profs_OCR_factoid.xlsx'\n",
        "writer = pd.ExcelWriter(workbook, engine='xlsxwriter') # create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "event_df.to_excel(writer, sheet_name='OCR-to-factoid') # Convert the dataframe to an XlsxWriter Excel object.\n",
        "writer.save() # Close the Pandas Excel writer and output the Excel file."
      ],
      "metadata": {
        "id": "XX8YSaovbPPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matches found in the text are displayed in the \"MATCH: WORD\" format to help you track the live performance of the script.\n"
      ],
      "metadata": {
        "id": "TNFqD_PGXPmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output files and repeat process with refined ontology files if necessary.\n",
        "\n",
        "Script by Monika Barget, Maastricht/Mainz\n",
        "\n",
        "January 2023\n"
      ],
      "metadata": {
        "id": "GBSEVpnKXS_u"
      }
    }
  ]
}