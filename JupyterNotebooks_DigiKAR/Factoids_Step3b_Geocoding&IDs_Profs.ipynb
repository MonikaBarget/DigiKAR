{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4xq4Oa5/axonnwRkdn9z8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a script for **adding coordinates and project IDs** (e.g. person IDs) to a factoid table in the DigiKAR project. The geocoding is not performed from scratch; instead, coordinates are added from a previously geocoded table of place names.\n",
        "\n",
        "The package mainly uses the Pandas package in Python to read and manipulate EXCEL data as DataFrames. DataFrames are 2-dimensional data representations in rows and columns. They can be written to different file formats such as CSV, EXCEL, JSON or RDF.\n",
        "\n",
        "First of all, we need to connect this Colab notebook with your Google Drive and define the directory for input and output data.\n"
      ],
      "metadata": {
        "id": "S3ydZRhYATDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "directory=\"/content/drive/My Drive/Colab_DigiKAR/\""
      ],
      "metadata": {
        "id": "39qqRJOgZmPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the second step, we have to install additional Packages needed for working with CSV, EXCEL and DataFrames."
      ],
      "metadata": {
        "id": "dCAdylkZL9f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## install packages that are not part of Python's standard distribution\n",
        "\n",
        "!pip install xlsxwriter\n",
        "!pip install pandas\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "3d6OjjlTZ2ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 1**, we can import the packages to the script and load our data. Before merging the input files, names will be normalised as some have access spaces, capitalised surnames, or inverted first and last names.\n",
        "\n",
        "The combined data will be written to a new dataframe and displayed."
      ],
      "metadata": {
        "id": "agRRVOwiOWXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xlsxwriter\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "# path to input files\n",
        "\n",
        "factoid_paths=[\"https://github.com/ieg-dhr/DigiKAR/raw/main/Sample%20Data/Factoid-PROFS-consolidation-STEP3-events-split-v10.xlsx\"]\n",
        "\n",
        "# define dataframe for final output\n",
        "\n",
        "f_to_add=[]\n",
        "\n",
        "# structure of input files\n",
        "\n",
        "# obligatory columns in valid factoid list\n",
        "\n",
        "column_names = [\"factoid_ID\",\n",
        "                \"pers_ID\",\n",
        "                \"alternative_names\",\n",
        "                \"event_type\",\n",
        "                \"event_after-date\",\n",
        "                \"event_before-date\",\n",
        "                \"event_start\",\n",
        "                \"event_end\",\n",
        "                \"event_date\",\n",
        "                \"pers_title\",\n",
        "                \"pers_function\",\n",
        "                \"place_name\",\n",
        "                \"inst_name\",\n",
        "                \"rel_pers\",\n",
        "                \"source_quotations\",\n",
        "                \"additional_info\",\n",
        "                \"comment\",\n",
        "                \"info_dump\",\n",
        "                \"source\",\n",
        "                \"source_site\"]\n",
        "\n",
        "frame_list=[]\n",
        "for file in factoid_paths:\n",
        "    df = pd.read_excel(file, index_col=None, dtype=str) # axis=1, sort=False sheet_name='FactoidList'\n",
        "    df = df.fillna(\"n/a\") # replace empty fields for string\n",
        "    frame_list.append(df)\n",
        "\n",
        "f_unique = pd.concat(frame_list, axis=0, ignore_index=True, sort=False)\n",
        "\n",
        "print(\"There are \", len(f), \"items in your DataFrame!\")\n",
        "\n",
        "# delete all duplicate rows with exact matches\n",
        "\n",
        "display(f_unique)\n"
      ],
      "metadata": {
        "id": "zYkO-hJ7rxxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 2**, we add the person ID's from the person ontology list and also add coordinates from Geonames and Google.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pFS6ouhczRj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge input dataframe with dfs containing person IDs and geocoding\n",
        "# documentation: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\n",
        "\n",
        "## Read person IDs from Github\n",
        "## columns: pers_ID_MB, pers_name, alternative_names, Unnamed: 4, name_new_fs, pers_ID_FS,poi, pers_name_corr, freq\n",
        "infile1=\"https://github.com/ieg-dhr/DigiKAR/raw/main/OntologyFiles/Factoid_PersonNames_merged.xlsx\" # has to contain pers_name column!\n",
        "person_df = pd.read_excel(infile1)\n",
        "\n",
        "## Read geocoding from Github\n",
        "infile2=\"https://github.com/ieg-dhr/DigiKAR/raw/main/OntologyFiles/Ortsontologie_Geocoded_gepr%C3%BCft.xlsx\" # has to contain place_name column!\n",
        "geo_df = pd.read_excel(infile2)\n",
        "\n",
        "## merge dataframes horizontally\n",
        "\n",
        "merged_df1 = pd.merge(f_unique, geo_df, on='place_name', how='inner').fillna(\"n/a\")\n",
        "merged_df2 = pd.merge(merged_df1, person_df, on=('pers_name', \"alternative_names\"), how=\"inner\").fillna(\"n/a\")\n",
        "\n",
        "display(merged_df2)"
      ],
      "metadata": {
        "id": "zOiLsme_EycO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3** adds event values from the event value Python dictionary on Github.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "srtsoNW4Mfln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load external dictionary with EVENT VALUES\n",
        "# following method 2 on https://www.geeksforgeeks.org/how-to-read-dictionary-from-file-in-python/\n",
        "\n",
        "# importing the module\n",
        "import requests\n",
        "import ast\n",
        "\n",
        "master = \"https://raw.githubusercontent.com/ieg-dhr/DigiKAR/main/Data%20Categorisation/Event_value_dict.txt\" # add Sven's new mapping\n",
        "req = requests.get(master)\n",
        "req = req.text\n",
        "print(req)\n",
        "\n",
        "# reconstructing the data as a dictionary\n",
        "event_value_dict = ast.literal_eval(req)\n",
        "print(type(event_value_dict))\n",
        "\n",
        "# add event values from dict to data frame\n",
        "\n",
        "try:\n",
        "    test = event_value_dict[\"Aufschw√∂rung\"] # random test if valid dict\n",
        "    print(\"Value for chosen key: \", test)\n",
        "except:\n",
        "    print(\"Invalid dict structure!\")\n",
        "\n",
        "merged_df2['event_value'] = merged_df2['event_type'].map(event_value_dict) # optional: na_action='ignore'\n",
        "\n",
        "display(merged_df2)"
      ],
      "metadata": {
        "id": "Rmm2eaHvEx6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4** is to write the results to a single output file."
      ],
      "metadata": {
        "id": "8wRkCp062_jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write all results to new EXCEL file\n",
        "\n",
        "workbook=directory+'FACTOIDS_consolidated/Factoid_PROFS_v10_geocoded-with-IDs.xlsx'\n",
        "print(workbook)\n",
        "writer = pd.ExcelWriter(workbook, engine='xlsxwriter') # create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "merged_df2.to_excel(writer, sheet_name='FactCons') # Convert the dataframe to an XlsxWriter Excel object.\n",
        "writer.save() # Close the Pandas Excel writer and output the Excel file.\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "qmxPBH9Q3FY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output files and repeat process if necessary.\n",
        "\n",
        "Script by Monika Barget, Maastricht/Mainz\n",
        "\n",
        "June 2023\n"
      ],
      "metadata": {
        "id": "GBSEVpnKXS_u"
      }
    }
  ]
}